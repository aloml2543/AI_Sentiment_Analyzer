# -*- coding: utf-8 -*-
#!/usr/bin/env python
import re

POSLIST =['CC','CD','DT','EX','FW','IN','JJ','LS','MD','NN','RB','RP','TO','UH','VB','WP',
'WP$','WRB','VBD','VBG','VBN','VBP','VBZ','WDT','NNS','NNP','JJR','JJS','RBR','RBS','PDT',
'POS','PRP','PRP$','NNPS']
return_v=10

class dic:
    data ={"i":{'POS':'PRP','SC':0},
          'am':{'POS':'VBP','SC':0},
          'sorry':{'POS':'JJ','SC':-0.7},
          'you':{'POS':'PRP','SC':0},
          'are':{'POS':'VBP','SC':0},
          'not':{'POS':'RB','SC':'nn'},
          'feeling':{'POS':'VBG','SC':0.1},
          'well':{'POS':'RB','SC':0.7},
          'know':{'POS':'VBP','SC':0},
          'what':{'POS':'WP','SC':0},
          'mean':{'POS':'VBP','SC':0},
          'congratulations':{'POS':'UH','SC':0.9},
          'done':{'POS':'VBN','SC':0.1},
          'really':{'POS':'RB','SC':'*1.5'},
          'for':{'POS':'IN','SC':0},
            'have':{'POS':('VBP','VB'),'SC':0,'CON':'''global return_v\nif 'done' in token_list:\n    return_v=1\nelse:\n    return_v=0'''}, #Ï°∞Í±¥  ... have... done....     ....have....
          'fun':{'POS':'NN','SC':0.8},
          'your':{'POS':'PRP$','SC':0},
          'lesson':{'POS':'NN','SC':0},
          'survived':{'POS':'VBD','SC':0.7},
          'but':{'POS':'CC','SC':0},
          'everything':{'POS':'NN','SC':0},
          'is':{'POS':'VBZ','SC':0},
          'hurting':{'POS':'VBG','SC':-0.7},
          'now':{'POS':'RB','SC':0},
          'it':{'POS':'PRP','SC':0},
          'took':{'POS':'VBD','SC':0},
          'over':{'POS':'IN','SC':0},
          '4':{'POS':'CD','SC':0},
          'hours':{'POS':'NNS','SC':0},
          'auts':{'POS':'UH','SC':-0.7},
          'bet':{'POS':'VB','SC':0},
          'that':{'POS':'IN','SC':0},
          'tomorrow':{'POS':'NN','SC':0},
          'will':{'POS':'MD','SC':0},
          'be':{'POS':'VB','SC':0},
          'a':{'POS':'DT','SC':0},
          'better':{'POS':'JJR','SC':0.7},
          'day':{'POS':'NN','SC':0},
          '?':{'POS':'PUC','SC':0},
          '!':{'POS':'PUC','SC':0},
          #'!!!!':{'POS':'UH','SC':0},         
          'during':{'POS':'IN','SC':0},
          '.':{'POS':'PUC','SC':0},
          ',':{'POS':'CC','SC':0},
          '...':{'POS':'PUC','SC':0},
          'üëè':{'POS':'EMO','SC':'POS','NAME':'CLAPPING','UNICODE':'U+1F44F'},
          'üôà':{'POS':'EMO','SC':'NEG','NAME':'SEENOEVIL','UNICODE':'U+1F648'},
          'üé∑':{'POS':'EMO','SC':'NON','NAME':'SAXOPHONE','UNICODE':'U+1F3B7'},
          'üò±':{'POS':'EMO','SC':'NEG','NAME':'SCREAMING','UNICODE':'U+1F631'},
          'üò´':{'POS':'EMO','SC':'NEG','NAME':'DISTRAUGHT','UNICODE':'U+1F62B'},
          'üòç':{'POS':'EMO','SC':'POS','NAME':'SMILING','UNICODE':'U+1F60D'},
          ':(':{'POS':'EMO','SC':'NEG','NAME':'UNSMILING','UNICODE':''},
          ':)':{'POS':'EMO','SC':'NEG','NAME':'SMILING_emo','UNICODE':''},
          ';)':{'POS':'EMO','SC':'POS','NAME':'WINK','UNICODE':''}
          }
  
            
    def NEWgetPOS(self, token_list) :
        """
        ÌÜ†ÌÅ∞ÏùÑ Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú POSÎ•º Î¶¨ÌÑ¥Ìï¥Ï£ºÎäî Ìï®Ïàò
        ÎßåÏïΩ ÌÜ†ÌÅ∞(ÌÇ§) Í∞íÏù¥ ÏÇ¨Ï†ÑÏóê ÏóÜÎã§Î©¥ ÏÇ¨Ï†ÑÏóê Ï∂îÍ∞ÄÌïòÎäî Í≥ºÏ†ïÏùÑ Í±∞Ï≥êÏÑú POS Î¶¨ÌÑ¥ Í∞ÄÎä•
        
        """
        temp_str=""
        temp_list=[]
        for i in token_list:
           key_v=i.__str__().lower()
           
           if (self.data[key_v]['CON']!='') :
               
               temp_str=self.data[key_v]['CON']
             
               #print(temp_str)
               code = compile(temp_str, '<string>', 'exec')
               exec(code)
               #print(return_v)
               temp_list.append(self.data[key_v]['POS'][int(return_v)])
               #print("token : ",key_v," POS : ",self.data[key_v]['POS'])
           else:
               try :
                   temp_list.append(self.data[key_v]['POS'])
               except KeyError:
                    print("key error occured during taging")
                    in_error=input("do you want add token "+key_v+" to dictionary? (yes or no)")
                    newKEY=key_v
                    if in_error.lower() == 'yes':
                        newPOS=input("input new POS for new token \""+newKEY+"\" :")
                        while newPOS not in POSLIST:
                            newPOS=input("input new POS for new token \""+newKEY+"\" :")
                        
                        newSC=input("input new SC for new token \""+newKEY+"\" :")
                        
                        while int(newSC) not in range(-1,+1):
                            newSC=input("input correct new SC for new token \""+newKEY+"\" :")
                        newList={}
                        
                        newList.update(POS=newPOS)
                        
                        newList.update(SC=newSC)
                        
                        self.data.setdefault(newKEY,newList)
                        return self.data[key_v]['POS']
                        
                    elif  in_error.lower()=='no':
                        print("you selected no. program will be terminated")
                        exit()
               #print("token : ",key_v," POS : ",self.data[key_v]['POS'])
        #print(temp_list)
        return temp_list
       
    def getSC(self, token) :
        """
        ÌÜ†ÌÅ∞ÏùÑ Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú Í∞êÏ†ï Ï†êÏàò(SC)Î•º Î¶¨ÌÑ¥Ìï¥Ï£ºÎäî Ìï®Ïàò
        ÎßåÏïΩ Í∞êÏ†ï Ï†êÏàò(SC)Í∞íÏù¥ ÏÇ¨Ï†ÑÏóê ÏóÜÎã§Î©¥ 0ÏúºÎ°ú Î¶¨ÌÑ¥
        
        """
        try:
            return self.data[token]['SC']
        except KeyError:
            return 0
        
    def getNAME(self, token) :
        """
        ÌÜ†ÌÅ∞ÏùÑ Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú Ïù¥Î™®Ìã∞ÏΩò/Ïù¥Î™®ÏßÄÏùò Ïù¥Î¶Ñ(NAME)ÏùÑ Î¶¨ÌÑ¥Ìï¥Ï£ºÎäî Ìï®Ïàò
        ÎßåÏïΩ Ïù¥Î™®Ìã∞ÏΩò/Ïù¥Î™®ÏßÄÏùò Ïù¥Î¶Ñ(NAME)Í∞íÏù¥ ÏÇ¨Ï†ÑÏóê ÏóÜÎã§Î©¥ 'NULL' Î°ú Î¶¨ÌÑ¥
        
        """
        try:
            return self.data[token]['NAME']
        except KeyError:
            return 'NULL'
        
    def getUNICODE(self, token) :
        """
        ÌÜ†ÌÅ∞ÏùÑ Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú Ïù¥Î™®Ìã∞ÏΩò/Ïù¥Î™®ÏßÄÏùò Ïú†ÎãàÏΩîÎìúÎ•º(UNICODE)ÏùÑ Î¶¨ÌÑ¥Ìï¥Ï£ºÎäî Ìï®Ïàò
        ÎßåÏïΩ Ïù¥Î™®Ìã∞ÏΩò/Ïù¥Î™®ÏßÄÏùò Ïú†ÎãàÏΩîÎìú(UNICODE)Í∞íÏù¥ ÏÇ¨Ï†ÑÏóê ÏóÜÎã§Î©¥ 'NULL' Î°ú Î¶¨ÌÑ¥
        
        """
        try:
            return self.data[token]['UNICODE']
        except KeyError:
            return 'NULL'
    
    def searchDIC(self, token) :
        """
        ÌÜ†ÌÅ∞ÏùÑ Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú ÏÇ¨Ï†ÑÏóê Ìï¥Îãπ ÌÇ§Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
        ÎßåÏïΩ ÌÇ§ Í∞íÏù¥ Ï°¥Ïû¨Ïãú True / ÏóÜÏùÑ Ïãú False Î¶¨ÌÑ¥
        
        """
        temp = token
        if (temp in self.data):
            return True
        else :
            return False
    

def tokenizer(sentence):
   
   """
   Î¨∏Ïû•ÏùÑ Í≥µÎ∞±ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÌï†->Î¶¨Ïä§Ìä∏Î°ú Ï†ÄÏû•
   Î∂ÑÌï†Îêú Îç©Ïñ¥Î¶¨ Îã®ÏúÑÎ°ú, Ï†ïÍ∑úÏãù Ïù¥Ïö©ÌïòÏó¨ ÌïÑÏöîÌïú ÌÜ†ÌÅ∞ Î¶¨Ïä§Ìä∏Î°ú Ï†ÄÏû• -> ÏòÅÎã®Ïñ¥,ÌäπÏàòÎ¨∏Ïûê(!,.?),Ïù¥Î™®ÏßÄ,Ïù¥Î™®Ìã∞ÏΩò Îã®ÏúÑÎ°ú Î∂ÑÎ•ò
   Ï†ïÍ∑úÏãùÏúºÎ°ú ÌÜ†ÌÅ∞Ìôî ÌõÑÏóê Ï†ïÎ†¨ ÌïÑÏöî -> ÎîïÏÖîÎÑàÎ¶¨(temp_dic)ÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÌÜ†ÌÅ∞Ìôî Ïù¥Ï†Ñ ÏúÑÏπò ÌôïÏù∏ÌïòÏó¨ ÏúÑÏπò:ÌÇ§ ÌÜ†ÌÅ∞:Í∞í ÏúºÎ°ú Ï†ÄÏû•
   Ï†ïÎ†¨ ÏÇ¨Ïö©ÌïòÏó¨ ÏõêÎûò ÏúÑÏπòÎ°ú Ï†ÄÏû•
   """
   
   character_pattern=re.compile("[a-zA-Z]+",flags=re.UNICODE)
   puct_pattern=re.compile('[\U00000021\U0000002C\U0000002E\U0000003F]+', flags=re.UNICODE)
   emoji_pattern = re.compile("[\U0001F44F\U0001F648\U0001F3B7\U0001F631\U0001F62B\U0001F60D]+", flags=re.UNICODE)
   emoti_pattern = re.compile("[\U00000022-\U00000029\U0000002F-\U0000003B]+", flags=re.UNICODE)
   
   def sorter(character_pattern,temp_str,start,temp1,temp_split):
       if character_pattern.search(temp_str) != None:
         temp_split+=character_pattern.findall(temp_str) 
        
   
   temp=[]
   splited = sentence.split(' ')
   #print("splited : ", splited)
   for index in range(0,len(splited)):
      temp_str=splited[index].__str__()
      start=0
      temp1=[]   
      temp_split=[]
      temp_dic={}
      sorter(puct_pattern,temp_str,start,temp1,temp_split)    
      sorter(character_pattern,temp_str,start,temp1,temp_split)
      sorter(emoji_pattern,temp_str,start,temp1,temp_split)        
      sorter(emoti_pattern,temp_str,start,temp1,temp_split)  
               
      for j in range(0,len(temp_split)):
                 
        if temp_str.find(temp_split[j].__str__(),start) >=0:
            temp_dic.setdefault(temp_str.find(temp_split[j].__str__(),start),temp_split[j])
            if temp_str.count(temp_split[j])>=2:
                start=temp_str.find(temp_split[j].__str__(),start)+len(temp_split[j])
      
      sdict = sorted(temp_dic.items(), key= lambda item : item[0])
      
      for i in sdict:
          temp.append(i[1])
          
   return temp   


def tagger(token_list):
    """
    ÌÜ†ÌÅ∞ Î¶¨Ïä§Ìä∏Î•º Îß§Í∞úÎ≥ÄÏàòÎ°ú Î∞õÏïÑÏôÄÏÑú ÌäúÌîåÎ°ú ÏûÑÏãú Ï†ÄÏû•
    ÌäúÌîåÏóê Ï†ÄÏû•Îêú ÌÜ†ÌÅ∞ÏùÑ Ïù¥Ïö©ÌïòÏó¨ getPOS ÌÅ¥ÎûòÏä§ Î©îÏÜåÎìúÎ•º Ïù¥Ïö©ÌïòÏó¨ POS Î¶¨Ïä§Ìä∏ Ïûë
    """
    dic1=dic()
    temp=[]
    
    temp+=dic.NEWgetPOS(dic1, token_list)
    
    return temp


def make_tree(sentence):
    rule = {
        'NN':{'VP':'VP'},
        'DT':{'NP': 'NP'},
        'VP':{'VP': 'VP', 'PUC': 'VP', 'PP': 'VP','EMO': 'VP','SBAR':'VP','VBG':'VP','RB':'VP','CP':'VP'},
        'VB':{'NN': 'VP','NP':'VP','PP': 'VP'},
        'VBD':{'PP': 'VP'},
        'VBZ':{'VBG': 'VP'},
        'VBP':{'VBN': 'VP','ADJP':'VP','JJ':'VP','RB':'VP'},
        'PRP':{'VRP': 'VP','VBP': 'VP','VP':'S','VBD':'S',},
        'PRP$':{'NP': 'NP'},
        'WDT':{'VP': 'VP'},
        'MD':{'VP': 'VP'},
        'WP':{'S':'SBAR','VP':'SBAR'},
        'CC':{'VP': 'CP','CC':'CC','CP':'CP'},
        'CP':{'VBD': 'VP','CC':'CP'},
        'CD':{'NNS':'NP'},
        'JJR':{'NN':'NP'},
        'UH':{'PUC':'S','EMO':'S'},
        'RB':{'VP': 'VP', 'VBN': 'VP','JJ':'ADJP','RP':'PP','CP':'VP'},
        'EMO':{'NN': 'NP'},
        'IN':{'NP': 'PP','SBAR':'PP','VP':'PP'},
        'S':{'EMO': 'S','CP':'S','S':'S','VP':'S'}
    }
    output = [pos for pos in sentence]
    count = 0
    
    point_F, point_R = len(sentence)-2, len(sentence)-1
    pre_output_len = len(output)
    
    while(len(output) != 1):
        count +=1
        try:
            sum_pos = rule[output[point_F]][output[point_R]]
            output[point_F] = sum_pos
            del output[point_R]
            if pre_output_len != len(output):
                print(output)
                pre_output_len = len(output)
                count = 0
            point_F, point_R = len(output)-2, len(output)-1
            for i in output:
                if i not in ['VP', 'S']:
                    continue
            for i in output:
                i = 'S'

        except:
            pass
        point_F -= 1
        point_R -= 1
        if count == 1000: break
        if len(output) == 1 and output[0] == 'VP':
            output[0] = 'S'
            print(output)

    return output
          
          
"""
Î©îÏù∏Î™®Îìà
ÌååÏùºÏóê ÌïÑÏöîÌïú Î¨∏Ïû•ÏùÑ ÏûÖÎ†•
ÏÑ†ÌÉùÌõÑÏóê ÏûêÎèôÏúºÎ°ú ÌÜ†ÌÅ∞Ìôî Î∞è ÌÉúÍπÖ ÏßÑÌñâ
"""
sentence =[]
dic1=dic()
f = open("source.txt", 'r',encoding='utf-8-sig')
while True:
    line = f.readline()    
    if not line: break
    line.rstrip('\n')
    sentence.append(line.rstrip('\n'))
    
print('inputed sentences\n')
index =-1

for i in sentence:
    if i:
        index +=1
        print(index,": ",i,'\n')        
    elif not i and index ==-1:
        print("empty file")
        break
    else:
        break
select=0

if index != -1 :
   print('select sentence to analyze',end="")

   while True:
        temp_input = input('input = ')
        if int(temp_input)>=0 and int(temp_input)<=len(sentence):
            select=int(temp_input)
            break;
        else:
            print("input correctly")
   
   print("selceted sentence : ",sentence[select])
   
   token_list=tokenizer(sentence[select])
   
   
   print("token list : ",token_list)
   print("tag list  : ",tagger(token_list))
 
   
   
   
   
  
   


        
    
    